{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa4nq6FJUk/dT49hsKmJ2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingsleynwafor54/RemoteSkillHub_Python/blob/main/Classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Classification Model with ColumnTransformer and Scalers\n",
        "\n",
        "## üì¶ 1. Import Libraries\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "üìÇ **2. Load the Dataset**\n",
        "\n",
        "Let‚Äôs use a simple dataset for classification (like Titanic or a custom one).\n",
        "\n",
        "```python\n",
        "# Example dataset\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Age': [25, 45, 35, 50, 23, 30],\n",
        "    'Salary': [50000, 80000, 60000, 120000, 40000, 70000],\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Male'],\n",
        "    'Purchased': [0, 1, 0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "data\n",
        "```\n",
        "\n",
        "\n",
        "üß© 3. Split Features and Target\n",
        "```python\n",
        "X = data[['Age', 'Salary', 'Gender']]\n",
        "y = data['Purchased']\n",
        "```\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "###‚öôÔ∏è 5. Apply ColumnTransformer\n",
        "\n",
        "We‚Äôll handle numerical and categorical columns separately.\n",
        "\n",
        "### Define which columns to transform\n",
        "```python\n",
        "numeric_features = ['Age', 'Salary']\n",
        "categorical_features = ['Gender']\n",
        "```\n",
        "\n",
        "### Create transformers\n",
        "```python\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder()\n",
        "```\n",
        "### Combine with ColumnTransformer\n",
        "```python\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "```\n",
        "üßÆ 6. Build and Train the Model\n",
        "\n",
        "We‚Äôll use Logistic Regression as the classification model.\n",
        "\n",
        "### Create pipeline-like structure\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "üßæ 7. Evaluate the Model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```"
      ],
      "metadata": {
        "id": "No0SVx9Th6Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéõÔ∏è Hyperparameter Tuning in Machine Learning\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 1. What Are Hyperparameters?\n",
        "\n",
        "Hyperparameters are **configuration settings** used to control how a machine learning model learns.  \n",
        "They are **not learned automatically** from the data ‚Äî instead, **you set them manually** before training.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Example:\n",
        "- For **Logistic Regression**, `C` (regularization strength) and `solver` are hyperparameters.  \n",
        "- For **Random Forest**, parameters like `n_estimators`, `max_depth`, and `min_samples_split` are hyperparameters.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è 2. Hyperparameters vs Parameters\n",
        "\n",
        "| Type | Learned During Training? | Example |\n",
        "|------|----------------------------|----------|\n",
        "| **Parameters** | ‚úÖ Yes | Weights and biases in Logistic Regression |\n",
        "| **Hyperparameters** | ‚ùå No | Learning rate, regularization term, number of trees |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 3. Why Tune Hyperparameters?\n",
        "\n",
        "Tuning helps to:\n",
        "- Improve **model accuracy**\n",
        "- Prevent **overfitting/underfitting**\n",
        "- Achieve **better generalization** on unseen data\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 4. Common Hyperparameter Tuning Methods\n",
        "\n",
        "### üîπ **1. Manual Search**\n",
        "You pick a few values and test them manually.\n",
        "> Simple but inefficient.\n",
        "\n",
        "```python\n",
        "model = LogisticRegression(C=0.1)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "jf9TisL_lpwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üîπ 2. Grid Search (Exhaustive Search)\n",
        "\n",
        "##### Tries every possible combination of parameters from a grid you define.\n",
        "\n",
        "#####Ensures best combination is found, but can be slow.\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Create grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Best Parameters:\", grid_search.best_params_)\n",
        "print(\"üèÜ Best Score:\", grid_search.best_score_)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "üîπ 3. Random Search\n",
        "\n",
        "##### Randomly selects a subset of parameter combinations.\n",
        "\n",
        "##### Faster than GridSearchCV.\n",
        "\n",
        "##### Useful when you have many hyperparameters.\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 300),\n",
        "    'max_depth': randint(2, 10),\n",
        "    'min_samples_split': randint(2, 10)\n",
        "}\n",
        "\n",
        "# Create random search\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
        "                                   n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Best Parameters:\", random_search.best_params_)\n",
        "print(\"üèÜ Best Score:\", random_search.best_score_)\n",
        "\n",
        "üîπ 4. Bayesian Optimization (Advanced)\n",
        "\n",
        "Uses probability to find optimal parameters more intelligently.\n",
        "\n",
        "Implemented with libraries like:\n",
        "\n",
        "optuna\n",
        "\n",
        "hyperopt\n",
        "\n",
        "skopt\n",
        "\n",
        "‚ö° Efficient for complex models like XGBoost, LightGBM, or Deep Lear\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "M1DL664QmBMA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbQUSJJBgLpf"
      },
      "outputs": [],
      "source": []
    }
  ]
}