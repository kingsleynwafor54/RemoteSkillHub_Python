{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9SIDqNsMSSpZN41MxhaK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingsleynwafor54/RemoteSkillHub_Python/blob/main/Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cd44d1"
      },
      "source": [
        "üéØ Feature Selection in Machine Learning\n",
        "\n",
        "üß© 1. Why Feature Selection Matters\n",
        "\n",
        "Feature selection helps your model focus only on what truly matters.\n",
        "\n",
        "‚úÖ Benefits:\n",
        "\n",
        "üöÄ Improves model accuracy (removes noise)\n",
        "\n",
        "‚ö° Reduces computation time\n",
        "\n",
        "üß† Makes the model more interpretable\n",
        "\n",
        "üßØ Prevents overfitting (too many irrelevant variables)\n",
        "\n",
        "üß† 2. Feature Selection Methods (for Regression)\n",
        "\n",
        "We‚Äôll go from simple ‚Üí advanced.\n",
        "\n",
        "üîπ A. Correlation Analysis (Quick & Visual)\n",
        "\n",
        "Find which features are most correlated with your target variable.\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr = df.corr(numeric_only=True)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Features with high positive or negative correlation (close to ¬±1) with the target are useful.\n",
        "\n",
        "Features with correlation ‚âà 0 are less useful.\n",
        "\n",
        "üìò Example:\n",
        "\n",
        "| Feature         | Correlation | Importance  |\n",
        "|-----------------|-------------|-------------|\n",
        "| Annual Income   | +0.7        | ‚úÖ Important |\n",
        "| Gender          | 0.05        | ‚ùå Weak     |\n",
        "\n",
        "üîπ B. Using Model Coefficients (Linear Regression)\n",
        "\n",
        "Inspect feature coefficients after fitting a model.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "X = df[['Age', 'Annual Income (k$)', 'Gender']]\n",
        "y = df['Spending Score (1-100)']\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "coef_df\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Larger absolute coefficient ‚Üí stronger effect\n",
        "\n",
        "Coefficient close to 0 ‚Üí weaker influence\n",
        "\n",
        "üîπ C. Recursive Feature Elimination (RFE)\n",
        "\n",
        "Automatically ranks features by importance and selects the best ones.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "rfe = RFE(model, n_features_to_select=2)\n",
        "rfe.fit(X, y)\n",
        "\n",
        "for i in range(len(X.columns)):\n",
        "    print(f\"{X.columns[i]}: Selected={rfe.support_[i]}, Rank={rfe.ranking_[i]}\")\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "‚úÖ Selected = True ‚Üí Best features\n",
        "üí° Lower rank ‚Üí more important\n",
        "\n",
        "üîπ D. Using Feature Importance from Tree Models\n",
        "\n",
        "Even if using Linear Regression, you can check feature power with Random Forest.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(importances)\n",
        "\n",
        "importances.plot(kind='bar', title='Feature Importance (Random Forest)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "üîπ E. Statistical Tests (for Numeric Data)\n",
        "\n",
        "Use f_regression to check statistical significance.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k='all')\n",
        "selector.fit(X, y)\n",
        "\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'F-Score': selector.scores_,\n",
        "    'P-Value': selector.pvalues_\n",
        "}).sort_values(by='F-Score', ascending=False)\n",
        "\n",
        "feature_scores\n",
        "```\n",
        "\n",
        "‚úÖ Interpretation:\n",
        "\n",
        "High F-Score ‚Üí more relevant\n",
        "\n",
        "Low P-value (< 0.05) ‚Üí statistically significant\n",
        "\n",
        "üß≠ 3. How to Decide Practically\n",
        "\n",
        "| Method                     | When to Use                | Output                |\n",
        "|----------------------------|----------------------------|-----------------------|\n",
        "| Correlation Heatmap        | First look                 | Visual relationships  |\n",
        "| Model Coefficients         | Linear models              | Numeric importance    |\n",
        "| RFE                        | Regression or classification | Rank & select         |\n",
        "| Random Forest Importance   | Any model                  | Nonlinear relationships|\n",
        "| F-Test                     | Numeric regression         | Statistical significance|\n",
        "\n",
        "üß† 4. Example Decision\n",
        "\n",
        "| Feature       | Correlation | RFE Rank | Keep?       |\n",
        "|---------------|-------------|----------|-------------|\n",
        "| Age           | -0.6        | 2        | ‚úÖ          |\n",
        "| Annual Income | +0.8        | 1        | ‚úÖ          |\n",
        "| Gender        | 0.03        | 3        | ‚ùå          |\n",
        "\n",
        "‚úÖ Best features: Age, Annual Income\n",
        "\n",
        "‚ö° Pro Tip: Automate Feature Selection in a Pipeline\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('select', SelectKBest(score_func=f_regression, k=2)),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X, y)\n",
        "```\n",
        "\n",
        "‚úÖ This way, the model automatically selects the 2 best features every time you train."
      ]
    }
  ]
}